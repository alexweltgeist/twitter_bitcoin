{"cells":[{"cell_type":"markdown","source":["# Tweets abnehmen und Sentiment analyse anwenden\n\nDie Tweets aus dem Producer werden eingelesen und als PArquet abgelegt.\nDarauf wird via *textblob* eine Sentiment analyse gemacht und jedem Tweet ein Rating zugewiesen."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f52820d-6724-46fe-b3fc-7403886b930e"}}},{"cell_type":"code","source":[" !pip install textblob"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c39d2af-7476-41ff-b89e-1a145089a650"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Libraries einlesen\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom textblob import TextBlob\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9656c85-6681-4e40-b39b-eb46e2e1b7e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# nur zum testen: stream sichtbar machen\n\n# Create a local/private StreamingContext and SparkContext.\nssc = StreamingContext(sc, 5)   # window size = 5\nstream = ssc.socketTextStream(\"localhost\", 9997)\nstream.pprint()\n\n# Start the computation with timeout function\n try:\n  ssc.start()                             \n  ssc.awaitTerminationOrTimeout(20)  # Ausgabe im consumer erst nach timeout m√∂glich (sekunden)\n finally:\n  ssc.stop(False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a5c3d2e-7d85-4d76-b8ce-1cfe5e0fce1d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Tokenize and preprocess text\n\ndef preprocessing(lines):\n    words = lines.select(explode(split(lines.value, \"t_end\")).alias(\"word\"))\n    words = words.na.replace('', None)\n    words = words.na.drop()\n    words = words.withColumn('word', F.regexp_replace('word', r'http\\S+', ''))\n    words = words.withColumn('word', F.regexp_replace('word', '@\\w+', ''))\n    words = words.withColumn('word', F.regexp_replace('word', '#', ''))\n    words = words.withColumn('word', F.regexp_replace('word', 'RT', ''))\n    words = words.withColumn('word', F.regexp_replace('word', ':', ''))\n    return words\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b444e526-dada-47c5-94ec-3826cd52b561"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Apply text classification via Textblob\n\ndef polarity_detection(text):\n    return TextBlob(text).sentiment.polarity\n  \ndef subjectivity_detection(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef text_classification(words):\n    # polarity detection\n    polarity_detection_udf = udf(polarity_detection, StringType())\n    words = words.withColumn(\"polarity\", polarity_detection_udf(\"word\"))\n    # subjectivity detection\n    subjectivity_detection_udf = udf(subjectivity_detection, StringType())\n    words = words.withColumn(\"subjectivity\", subjectivity_detection_udf(\"word\"))\n    return words\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5b73e0b-1423-4556-808f-096905f53c85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Load in the Tweets and apply textblob sentiment\n\nif __name__ == \"__main__\":\n    # create Spark session\n    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n\n    # read the tweet data from socket\n    lines = spark.readStream.format(\"socket\").option(\"host\", 'localhost').option(\"port\", 9997).load()\n    # Preprocess the data\n    words = preprocessing(lines)\n    # text classification to define polarity and subjectivity\n    words = text_classification(words)\n\n    #write tweets into parquet files every 60 sec\n    words = words.repartition(1)\n    query = words.writeStream.queryName(\"all_tweets\")\\\n        .outputMode(\"append\").format(\"parquet\")\\\n        .option(\"path\", \"dbfs:/FileStore/bd_project\")\\\n        .option(\"checkpointLocation\", \"./check\")\\\n        .trigger(processingTime='60 seconds').start()\n    query.awaitTermination()\n   "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f50a1f1-8a67-430c-8e12-dae76757d446"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Twitter_Project_Consumer_v01","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2959400940596462}},"nbformat":4,"nbformat_minor":0}
